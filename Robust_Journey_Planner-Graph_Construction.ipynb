{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.5.0-spark2.1-s_2.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import os.path\n",
    "import getpass\n",
    "import pyspark\n",
    "import itertools\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from datetime import *\n",
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, Window\n",
    "\n",
    "from collections import Counter\n",
    "from dateutil.parser import parse\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName('sbb_planner-{0}'.format(getpass.getuser())) \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.executor.instances', '5') \\\n",
    "    .config('spark.port.maxRetries', '100') \\\n",
    "    .config('spark.jars.packages', 'graphframes:graphframes:0.6.0-spark2.3-s_2.11')\\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Journey planner - Graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess the dataset\n",
    "### 1.1 Load the dataset\n",
    "To tackle the problem, we decided to use 3 different subsets of the whole data set. First we used only the data of january 2019 to extract all the different routes and timetables, then we used the data from december 2018 to validate the routes (see that they existed aswell in december 2018) and finally we used the whole dataset to extract the different probability distributions for each of our clustered routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the year to compute validation dataframe\n",
    "def load_data(subset=True, date=\"2019/01\"):\n",
    "    \"\"\" Loads only data from date unless subset is set to False, then loads all.\"\"\"\n",
    "    if(subset):\n",
    "        df = spark.read.option(\"header\", \"true\").csv(\"/datasets/sbb/{:}*\".format(date), sep=\";\")\n",
    "    else:\n",
    "        df = spark.read.option(\"header\", \"true\").csv(\"/datasets/sbb/*/*/*\", sep=\";\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset according to the definition of the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load SBB dataset\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess the dataset\n",
    "We rename the columns to have the names in english instead of german and finally we format the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and format the columns\n",
    "df_sbb = rename_columns(df)\n",
    "df_sbb = format_columns(df_sbb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Add metadata and filter\n",
    "After this, let's get a dataframe that contains metadata. More precisely, we are intersted in the  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met = spark.read.text(\"Ressources/BFKOORD_GEO\")\n",
    "df_metadata = get_metadata(df_met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance calculator and filter\n",
    "\n",
    "As stated in the problem description, we filter out stations that are more than 10 km away from Zurich Hauptbanhof. We use the coordinates in the metadata to implement a function that determines the distance from a given station to the Zurich HB. We have been helped in this task by this website:\n",
    "https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zurich latitude and longitude \n",
    "y_zhb, x_zhb = 47.37743, 8.540192\n",
    "# Approximate radius of earth in km\n",
    "R = 6373.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to(y1, x1, y2=y_zhb, x2=x_zhb):\n",
    "    \"\"\" This function computes the distances from a point\n",
    "        (x1, x2) to Zurich HB. \"\"\"\n",
    "    lat1, lon1 = radians(y2), radians(x2)\n",
    "    lat2, lon2 = radians(y1), radians(x1)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the udf for the above function\n",
    "distance_udf = udf(lambda y,x: distance_to(y, x), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter out entries of stations that are more than 10k and save the dataframe\n",
    "df_metadata_filter = df_metadata.withColumn(\"distance\", distance_udf(\"y\", \"x\")).filter(col(\"distance\") <= 10)\n",
    "df_metadata_filter.toPandas().to_pickle('Ressources/metadata')\n",
    "# Join filtered stations with the original dataset we have\n",
    "df_sbb = df_sbb.join(df_metadata_filter, on=\"STOP_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the dataframe containing the distinct lines and save it\n",
    "df_coords = df_sbb.filter((col('OPERATING_DAY') == pd.Timestamp(2019, 1, 18)))\\\n",
    "                  .select(\"x\", \"y\", \"STOP_NAME\", \"LINE_TEXT\", \"LINE_ID\", \"TRIP_ID\", \"ARRIVAL_TIME\", \"DEPARTURE_TIME\")\\\n",
    "                  .toPandas()\n",
    "df_lines_unique = get_distinct_lines(df_coords)\n",
    "df_lines_unique.to_pickle('Ressources/unique_lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Deal with redundant and dirty data\n",
    "We filter out entries where the transport does not stop, also when it is an additional or a cancelled trip. We also need to take into account the forecast status columns. We decided to approach the problem initally by replacing the null entries in the \n",
    "estimated arrival and departure by the scheduled departure and arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbb = df_sbb.filter(~(df_sbb.DRIVE_THROUGH | df_sbb.ADDITIONAL_TRIP | df_sbb.CANCELLED_TRIP)) \\\n",
    "               .drop(\"DRIVE_THROUGH\", \"ADDITIONAL_TRIP\", \"CANCELLED_TRIP\", \"id\", \"BPUIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with the null values in the dates\n",
    "null_date = parse('1994-01-01 00:00:00')\n",
    "df_sbb = df_sbb.withColumn('ARRIVAL_TIME', when(df_sbb.ARRIVAL_TIME.isNull(), null_date)\\\n",
    "                                          .otherwise(df_sbb.ARRIVAL_TIME))\\\n",
    "               .withColumn('DEPARTURE_TIME', when(df_sbb.DEPARTURE_TIME.isNull(), null_date)\\\n",
    "                                            .otherwise(df_sbb.DEPARTURE_TIME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lambda calculation\n",
    "Considering that the probability distribution of the delay of a given transport is similar to a poisson distribution, we decide to calculate the different lambdas for each trip. We take into account the following when we want to estimate the distribution:\n",
    "- Departing Station\n",
    "- Arrival Station\n",
    "- Day of the week (weekday or not)\n",
    "- Rush hour or not (7am-9a. and 5pm-6pm)\n",
    "\n",
    "Once we have grouped the data given a line id and operator, we separate it given the stated variables and produce a dataframe containing the different lambda values for each line.\n",
    "\n",
    "We will add two columns to our dataframe where we indicate if it is a weekday or not and if it is rush hour or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in estimated arrival by scheduled arrival time\n",
    "df_sbb = df_sbb.withColumn(\"AT_FORECAST\", coalesce(df_sbb.AT_FORECAST, df_sbb.ARRIVAL_TIME))\n",
    "df_sbb = df_sbb.withColumn(\"DT_FORECAST\", coalesce(df_sbb.DT_FORECAST, df_sbb.DEPARTURE_TIME))\n",
    "# Add hour column\n",
    "df_sbb = df_sbb.withColumn(\"hour\", hour(\"ARRIVAL_TIME\").cast(IntegerType()))\n",
    "# Add rush hour indicator column\n",
    "df_sbb = df_sbb.withColumn(\"rush\", ((col(\"hour\") >= lit(7)) & (col(\"hour\") < lit(9))) \n",
    "                                 | ((col(\"hour\") >= lit(17)) & (col(\"hour\") < lit(18))))\n",
    "# Add weekday indicator column\n",
    "df_sbb = df_sbb.withColumn(\"weekday\", date_format('OPERATING_DAY', 'u') <= 5)\n",
    "# Filter out lines where there is no arrival\n",
    "df_sbb_arrivals = df_sbb.filter(df_sbb.AT_FORECAST.isNotNull() & df_sbb.ARRIVAL_TIME.isNotNull())\n",
    "# Compute difference between actual arrival and scheduled one\n",
    "df_sbb_arrivals = df_sbb_arrivals.withColumn(\"delay\", \n",
    "                                             unix_timestamp(\"AT_FORECAST\").cast(FloatType()) \n",
    "                                             - unix_timestamp(\"ARRIVAL_TIME\").cast(FloatType()))\n",
    "# Function to map negative and null delays to get a good lambda value\n",
    "correct_neg_null = udf(lambda diff: 0.0001 if diff <= 0 else diff, FloatType())\n",
    "# Add delay\n",
    "df_sbb_arrivals = df_sbb_arrivals.withColumn(\"delay\", correct_neg_null(\"delay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbb_arrivals.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute all the lambdas for the arrival times at each station, this is a long computation. After doing it, values should be stored in a parquet file to avoid recomputing every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lambdas = df_sbb_arrivals.groupBy(\"LINE_ID\", \"LINE_TEXT\", \"STOP_NAME\", \"weekday\", \"rush\")\\\n",
    "                            .agg({\"delay\": \"mean\", \"hour\": \"count\"})\\\n",
    "                            .withColumnRenamed(\"avg(delay)\", \"lambda\")\\\n",
    "                            .withColumnRenamed(\"count(hour)\", \"count\")\n",
    "df_lambdas.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if parquet file missing\n",
    "#df_lambdas.write.parquet(\"lambda.parquet\")\n",
    "df_l = spark.read.parquet(\"lambda.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We join the lambdas and the original dataset. Now we have a dataset with the estimated delay for each line, need to consider if we change the lambda for lines where we dont have many entries or what should we do. Probably drop additional trip lines and cancelled trips.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbb_lambda = df_sbb.join(df_l, (df_l.STOP_NAME == df_sbb.STOP_NAME) & \n",
    "                                  (df_l.LINE_ID == df_sbb.LINE_ID) & \n",
    "                                  (df_l.LINE_TEXT == df_sbb.LINE_TEXT) &\n",
    "                                  (df_l.weekday == df_sbb.weekday) &\n",
    "                                  (df_l.rush == df_sbb.rush))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the additional trips and cancelled trips\n",
    "df_sbb_lambda = df_sbb_lambda.filter(col(\"count\") > 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Route computation for every day\n",
    "Here we use the arrival time of each (trip_id, day) pair to compute the different routes in chronological order by taking advantage of the the null date. We order by the arrival time, hence the station with the null date will be the first in the list. To do this we implement a window.\n",
    "\n",
    "We also need to consider the case when the trip only covers 1 station in the zurich radius, hence if the number of stations in a trip is smaller than 2 we should not consider this line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_trip_day = Window.partitionBy(\"OPERATING_DAY\", \"TRIP_ID\").orderBy(asc(\"ARRIVAL_TIME\"))\n",
    "window_total_stops = Window.partitionBy(\"TRIP_ID\", \"OPERATING_DAY\")\n",
    "stop_num = rank().over(window_trip_day).alias(\"stop_num\")\n",
    "num_stops = functions.max(\"stop_num\").over(window_total_stops).alias(\"total_stops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the stop numbers and filter the trips where there is only one stop considered\n",
    "df_sbb = df_sbb.withColumn(\"stop_num\", stop_num)\\\n",
    "               .withColumn(\"total_stops\", num_stops) \\\n",
    "               .filter(col(\"total_stops\") > 1).drop(\"total_stops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Compute the time difference between 2 stations\n",
    "\n",
    "We need to compute the time it takes to go from one station to the next in a given trip, to do this we use the arrival time of the next station minus the departure time of the current station for a given trip_id, operating_day pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do a rolling window to compute the time difference\n",
    "next_time_arrival = lead(\"ARRIVAL_TIME\").over(window_trip_day).alias(\"next_time_arrival\")\n",
    "next_stop_f = lead(\"STOP_NAME\").over(window_trip_day).alias(\"next_stop_name\")\n",
    "# Add to each row the time of arrival to the next station\n",
    "df_sbb = df_sbb.select(\"*\", next_time_arrival, next_stop_f)\n",
    "df_sbb = df_sbb.withColumn(\"time_to_next\", unix_timestamp(df_sbb.next_time_arrival) - \n",
    "                           unix_timestamp(df_sbb.DEPARTURE_TIME))\n",
    "# Keep only valid data\n",
    "df_sbb = df_sbb.filter(df_sbb.time_to_next.isNotNull() & (df_sbb.time_to_next >= 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transform departure/arrival times to HH:mm format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_transform = [\"DEPARTURE_TIME\", \"DT_FORECAST\", \"ARRIVAL_TIME\", \"AT_FORECAST\", \"next_time_arrival\"]\n",
    "df_sbb_times = df_sbb.withColumn('weekday_num', functions.dayofweek(\"OPERATING_DAY\"))\n",
    "for time in columns_to_transform:\n",
    "    df_sbb_times = df_sbb_times.withColumn(time, date_format(time, \"HH:mm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create a working structure to find the shortest path\n",
    "In order to compute the different paths we want from A to B, we need to construct a proper data structure where we can look at the different possible immediate movements. Immediate movements meaning the a direct trip from two adjacent stations.\n",
    "\n",
    "We already created the different lambda list for the arrival delays by grouping the different possible trips by:\n",
    "- \"LINE_ID\", \"LINE_TEXT\"\n",
    "- \"STOP_NAME\", \n",
    "- \"weekday\", (here need to specify also for sunday as timetables are different)\n",
    "- \"rush\"\n",
    "\n",
    "And we'll do the same here. These with the addition of __the next stop__ will be the __index values__ for our data structure and in the different column values we will have:\n",
    "- ``departures`` (all possible times during the day)\n",
    "- ``next_arrivals`` (all possible arrival times during the day for the next stop)\n",
    "- ``the time to the next station`` (we will take the median time of all the connexions from A to B)\n",
    "\n",
    "The departures and the arrivals need to be sorted. To get the described datastructure we will also need to initially group by the operating day so that we dont get duplicates of a particular trip. After that we will group by the same values except the operating day and for the multiple lists we will take the one considered to be the median.\n",
    "\n",
    "After having this datastructure we will also be able to link it with the uncertainty values in the lambda table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_connexions = df_sbb_times.groupBy(\"STOP_NAME\", \"next_stop_name\", 'PRODUCT_ID', \n",
    "                                     \"LINE_ID\", \"LINE_TEXT\", \"weekday_num\")\\\n",
    "                            .agg(collect_list(\"DEPARTURE_TIME\").alias(\"departures\"),\n",
    "                                 collect_list(\"next_time_arrival\").alias(\"next_arrivals\"),\n",
    "                                 collect_list(\"time_to_next\").alias(\"time_to_next_list\")) \n",
    "df_connexions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort arrays departures and next_arrivals\n",
    "df_connexions = df_connexions.withColumn(\"departures\", sort_array(\"departures\"))\\\n",
    "                             .withColumn(\"next_arrivals\", sort_array(\"next_arrivals\"))\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this preprocessing, we can finally transform our data to pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert data to dataframes\n",
    "dfp_connexions = df_connexions.toPandas()\n",
    "dfp_lambda = df_l.toPandas()\n",
    "dfp_metadata = df_metadata_filter.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the lambda by count and save the dataframe\n",
    "dfp_lambda_filtered = dfp_lambda[dfp_lambda[\"count\"] > 50].drop(columns=[\"count\"])\n",
    "dfp_lambda_filtered.to_pickle('Ressources/lambdas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Prepare data for Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the edges by recurrence and make them unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_edges(row):\n",
    "    \"\"\" This function unites departure/arrival/time columns in one \"\"\"\n",
    "    s = set()\n",
    "    for (d, a, t) in zip(row['departures'], row['next_arrivals'], row['time_to_next_list']) :\n",
    "        time_d = to_time(d)\n",
    "        time_a = to_time(a)\n",
    "        if time_d <= time_a:\n",
    "            s.add((time_d, time_a, t))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the smallest number of trip appearance\n",
    "MIN_NUMBER_REAL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter null values and links to itself\n",
    "dfp_connexions_f = dfp_connexions[(dfp_connexions['STOP_NAME'].notna()) & (dfp_connexions['next_stop_name'].notna())]\n",
    "dfp_connexions_f = dfp_connexions_f[(dfp_connexions_f['STOP_NAME'] != dfp_connexions_f['next_stop_name'])]\n",
    "# We take only trips that appear many times. The other one might just be extraordinary trips\n",
    "dfp_connexions_f = dfp_connexions_f[dfp_connexions_f['departures'].apply(lambda l : len(l) >= MIN_NUMBER_REAL)]\n",
    "# Create edges for the graph\n",
    "dfp_connexions_f['edges'] = dfp_connexions_f.apply(make_edges , axis = 1)\n",
    "dfp_connexions_f['edges'] = dfp_connexions_f['edges'].apply(list)\n",
    "dfp_connexions_f = dfp_connexions_f.drop(columns = ['departures', 'next_arrivals', 'time_to_next_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we flatten the dataframe in order to have only 1 row per trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfp_flatten = dfp_connexions_f.edges.apply(pd.Series) \\\n",
    "                              .merge(dfp_connexions_f, right_index = True, left_index = True) \\\n",
    "                              .drop([\"edges\"], axis = 1) \\\n",
    "                              .melt(id_vars = ['STOP_NAME', 'next_stop_name', 'PRODUCT_ID', \n",
    "                                               'LINE_ID', 'LINE_TEXT', 'weekday_num'], value_name = \"edges\") \\\n",
    "                              .drop(\"variable\", axis = 1).dropna()\n",
    "dfp_flatten = dfp_flatten[dfp_flatten['edges'].apply(len) == 3]\n",
    "dfp_flatten['departure'] = dfp_flatten['edges'].apply(lambda e : e[0])\n",
    "dfp_flatten['arrival'] = dfp_flatten['edges'].apply(lambda e : e[1])\n",
    "dfp_flatten['time'] = dfp_flatten['edges'].apply(lambda e : e[2])\n",
    "dfp_flatten = dfp_flatten.sort_values(by = 'departure',axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Build edges\n",
    "\n",
    "#### 3.4.1 Transfer edges\n",
    "When going from a station to itself with a different line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_time(node1, node2):\n",
    "    \"\"\" This function defines the transfer time needed depending\n",
    "        on the different transport types changes. \"\"\"\n",
    "    old_type, new_type = node1[5], node2[5]\n",
    "    old_line_id, new_line_id = node1[3], node2[3]\n",
    "    old_line_text, new_line_text = node1[4], node2[4]\n",
    "    time = 0\n",
    "    # We check the changes in the type of transportation\n",
    "    if old_type == None:\n",
    "        time = 0   \n",
    "    elif old_type == new_type == 'Zug':\n",
    "        time = 2 \n",
    "    elif old_type == new_type == 'Bus': \n",
    "        time = 1\n",
    "    elif old_type == new_type == 'Tram': \n",
    "        time = 1   \n",
    "    elif old_type == new_type:\n",
    "        time = 2    \n",
    "    else :\n",
    "        time = 3\n",
    "    # We check whether the line_id and line_text are the same\n",
    "    if old_line_id == new_line_id and old_line_text == new_line_text:\n",
    "        time = 0\n",
    "    return timedelta(minutes = time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['LINE_ID', 'LINE_TEXT', 'PRODUCT_ID', 'departure']\n",
    "dfp_departures = dfp_flatten.groupby(['STOP_NAME', 'weekday_num'])[cols].agg(list).reset_index()\n",
    "cols = ['LINE_ID', 'LINE_TEXT', 'PRODUCT_ID', 'arrival']\n",
    "dfp_arrivals = dfp_flatten.groupby(['next_stop_name', 'weekday_num'])[cols].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_connex_joined = dfp_arrivals.merge(dfp_departures,\n",
    "                                      left_on=['next_stop_name', 'weekday_num'],\n",
    "                                      right_on = ['STOP_NAME', 'weekday_num'],\n",
    "                                      suffixes=('_arr','_dep'))\\\n",
    "                               .drop(columns = ['next_stop_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Walking time edges\n",
    "We consider it is possible to do a connexion between different lines by walking when the distance is not too big. Here for simplicity we will consider the distance bewteen two stations is walkable when the distance is smaller or equal to 1km. Also we consider the average human walking speed of 5 km/h. We need to create a dataframe with the walking distances between stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfp_walking = dfp_metadata.drop(columns=[\"distance\", \"id\"])\n",
    "dfp_walking['join'] = 1\n",
    "dfp_walking = dfp_walking.set_index(\"join\")\n",
    "dfp_pairs = dfp_walking.join(dfp_walking, lsuffix=\"\", rsuffix=\"_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def station_pair(row):\n",
    "    if row[\"STOP_NAME\"] > row[\"STOP_NAME_2\"]:\n",
    "        return row[\"STOP_NAME\"] + \", \" + row[\"STOP_NAME_2\"]\n",
    "    return row[\"STOP_NAME_2\"] + \", \" + row[\"STOP_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_pairs[\"stop_pair\"] = dfp_pairs.apply(station_pair, axis=1)\n",
    "dfp_pairs = dfp_pairs.drop_duplicates(subset=\"stop_pair\").drop([\"z\", \"z_2\"], axis=1)\n",
    "dfp_pairs = dfp_pairs.reset_index().drop(columns=[\"stop_pair\", \"join\"])\n",
    "# Filter out pairs with same stop name\n",
    "dfp_pairs = dfp_pairs[dfp_pairs.STOP_NAME != dfp_pairs.STOP_NAME_2]\n",
    "# Get distance between pair of stations\n",
    "dfp_pairs[\"dist\"] = dfp_pairs.apply(lambda row: distance_to(row[\"y\"], row[\"x\"], row[\"y_2\"], row[\"x_2\"]), axis=1)\n",
    "# Drop pairs where distance is smaller than 1 km\n",
    "dfp_pairs = dfp_pairs[dfp_pairs[\"dist\"] <= 1]\n",
    "dfp_pairs[\"walking_time_secs\"] = dfp_pairs[\"dist\"] * 60*60 / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Compute graph\n",
    "Each node is specified by (Stop name, day of week, Time, Line id, Line Text, Transport Type)\n",
    "\n",
    "Each edge is specified by (from_node, to_node,  {\n",
    "- 'trip' : The time of the trip (will be non null if it is in a public transport)\n",
    "- 'wait' : Waiting time at station (will be non null if it is from one station to itself but later)\n",
    "- 'walk' : Transfer time inside a station (walking from one train to other, or bus to train...)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def construct_graph(graph_filename):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # First compute all the edges that are a trip (using Bus, train, tram...)\n",
    "    trip_edges_to_add = []\n",
    "    i = 0\n",
    "    size_trips = len(dfp_flatten)\n",
    "    for index, r in dfp_flatten.iterrows():\n",
    "        e = r['edges'] \n",
    "        node_from = (r['STOP_NAME'], r['weekday_num'], e[0], r['LINE_ID'], r['LINE_TEXT'], r['PRODUCT_ID'])\n",
    "        node_to = (r['next_stop_name'], r['weekday_num'], e[1], r['LINE_ID'], r['LINE_TEXT'], r['PRODUCT_ID'])\n",
    "        labels = {'trip' : e[2], 'wait' : 0, 'walk' : 0}\n",
    "        trip_edges_to_add.append((node_from, node_to, labels))\n",
    "        i += 1\n",
    "        if i%1000 == 0:\n",
    "            print(\"Building trips: %.2f%%\" % (i*100/size_trips), end= '\\r')\n",
    "            sys.stdout.flush()\n",
    "    print(\"Adding trip edges...\")\n",
    "    G.add_edges_from(trip_edges_to_add)\n",
    "    \n",
    "    name2nodes = get_name_to_nodes(G)\n",
    "    \n",
    "    # Second add all the edges that are a transfer/waiting time (same station, different line)\n",
    "    transfer_edges_to_add = []\n",
    "    j = 0\n",
    "    size_transfers = len(G.nodes())\n",
    "    for node_from in G.nodes():\n",
    "        nodes_time = get_nodes_time(node_from[0], node_from[2], add_timedelta(node_from[2], timedelta(seconds=600)),\n",
    "                                    node_from[1], name2nodes)\n",
    "        for node_to in nodes_time:\n",
    "            trans_time = transfer_time(node_from, node_to)\n",
    "            if(add_timedelta(node_from[2],trans_time) <= node_to[2]):\n",
    "                wait_time = substract_times(node_to[2], add_timedelta(node_from[2], trans_time)).seconds\n",
    "                labels = {'trip' : 0, 'wait' : wait_time, 'walk' : trans_time.seconds}   \n",
    "                transfer_edges_to_add.append((node_from, node_to, labels))\n",
    "        j += 1\n",
    "        if j%1000 == 0:\n",
    "            print(\"Building transfers/waitings: %.2f%%\" % (j*100/size_transfers), end= '\\r')\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    print(\"Adding transfer edges...\")\n",
    "    G.add_edges_from(transfer_edges_to_add)\n",
    "                \n",
    "    print('Saving...')\n",
    "    with open(graph_filename, 'wb') as handle:\n",
    "        pickle.dump(G, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_filename = 'Ressources/graph.pickle'\n",
    "try:\n",
    "    f = open(graph_filename, 'rb')\n",
    "    print(\"Loading Graph...\")\n",
    "    with f as handle:\n",
    "        G = pickle.load(handle)  \n",
    "    print(\"Done\")\n",
    "except IOError:\n",
    "    G = construct_graph(graph_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of nodes: {}'.format(len(G.nodes)))\n",
    "print('Number of edges: {}'.format(len(G.edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, you can now use Robust_Journey_Planner-Results_Vizualisation.ipynb that loads the Graphe built and plot the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
